{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 4700 - Homework \\#3, Part 1\n",
    "**Due Date: Friday, 12/7 @ 6pm on CMS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will be implementing an (artificial) **neural network**, training and testing it in Python 3. The assignment requires an additional library to handle plotting the graphs. If you do not have `matplotlib` installed on your machine, please run the following commands to do so:\n",
    "\n",
    "`python3 -m pip install -U pip` <br/>\n",
    "`python3 -m pip install -U matplotlib`\n",
    "\n",
    "For more information about this package (and possible troubleshooting if the above installation does not work), please visit their [website](https://matplotlib.org/). Once you have it successfully installed, let's start by importing it (and others) into the notebook. Note, these are the only `import` statements allowed for this assignment; anything else will result in a penalty!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin the assignment by first defining what exactly is a neural network (NN). Recall from lecture that a NN is a collection of **units** or **nodes**, each of which uses an activation function to propagate its input signals as an output signal. These units are connected via weighted links. Every unit also has a bias term that can be represented as a dummy input with a fixed output value of 1. For simplicity, we will assume our NNs are **feed-forward NNs**, which can be represented as a directed acyclic graph of units.\n",
    "\n",
    "The code cell below defines the `NNUnit` class, which will represent a unit in a NN. Creating an actual NN is handled by the function `createNN`. Start the assignment by reading through and understanding the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A unit of a neural network.\n",
    "\"\"\"\n",
    "class NNUnit:\n",
    "    def __init__(self, activation, inputs = None, weights = None):\n",
    "        self.activation = activation\n",
    "        self.inputs = inputs or []\n",
    "        self.weights = weights or []\n",
    "        self.value = None\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Creates a neural network.\n",
    "\"\"\"\n",
    "def createNN(input_layer_size, hidden_layer_sizes, output_layer_size, activation):\n",
    "    layer_sizes = [input_layer_size] + hidden_layer_sizes + [output_layer_size]\n",
    "    network = [[NNUnit(activation) for _ in range(s)] for s in layer_sizes]\n",
    "    dummy_node = NNUnit(activation)\n",
    "    dummy_node.value = 1.0\n",
    "    \n",
    "    # create weighted links\n",
    "    for layer_idx in range(1, len(layer_sizes)):\n",
    "        for node in network[layer_idx]:\n",
    "            node.inputs.append(dummy_node)\n",
    "            node.weights.append(0.0)\n",
    "            for input_node in network[layer_idx - 1]:\n",
    "                node.inputs.append(input_node)\n",
    "                node.weights.append(0.0)\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we will need for our NN representation is an activation function. For now, let's use the sigmoid function, $\\sigma(z)$. The derivative of the sigmoid will become necessary for the next section. For reference, here are the equations:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma(z) &= \\frac{1}{1 + e^{-z}} \\\\\n",
    "\\frac{d}{dz}\\sigma(z) &= \\sigma(z) \\times (1 - \\sigma(z))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### <span style=\"color:blue\"> Q1.  Properties of Activation Functions </span>\n",
    "\n",
    "1. What would happen if we use the identity function, i.e., $\\sigma(z) := z$, as an activation function? Hint: what kind of functions can the network approximate? What is the effect of adding more layers?\n",
    "0. Consider the end behavior of the sigmoid function and its derivative --- as $z$ gets far away from zero, what happens? \n",
    "0. Interpret this in the context of a neural network: what does this mean for activations, and for the training process?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> P1.  Implement Sigmoid and Its Derivative </span>\n",
    "\n",
    "Note, for these two functions, you do **not** need to add documentation. The implementation should be very straight-forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def sigmoid_deriv(z):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Back-Propagation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you learned in lecture, we can teach our NN to learn by adjusting the weights of each link. These adjustments are derived by propagating backwards the error from the output layer. We will be running this back-propagation algorithm on the following NN:\n",
    "\n",
    "![](neural_net.png)\n",
    "\n",
    "Note that green represents input nodes, goldenrod represents hidden nodes, blue represents output nodes, and pink is the dummy node. Our goal is for this NN to learn the 2-bit adder function. The truth table for this operation is:\n",
    "\n",
    "| $x_1$ | $x_2$ | $y_1$<br/>(carry) | $y_2$<br/>(sum) |\n",
    "| :---: | :---: | :---: | :---: |\n",
    "| 0 | 0 | 0 | 0 |\n",
    "| 0 | 1 | 0 | 1 |\n",
    "| 1 | 0 | 0 | 1 |\n",
    "| 1 | 1 | 1 | 0 |\n",
    "\n",
    "For reference, here is the back-propagation algorithm in pseudo-code, taken from the textbook (fig. 18.24).\n",
    "![](back-prop_pseudo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Q2a.  Symbols </span>\n",
    "What do $a_i$, $\\Delta[i]$, and $w_{i,j}$ represent?\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Q2b.  Manual Back-Propagation </span>\n",
    "Before we implement the back-propagation algorithm, let's actually work out the math for entry 3 of the above data set (inputs are 1 and 0, outputs are 0 and 1). This will be useful for you in ensuring you understand the algorithm. You will perform one iteration of the algorithm, i.e., forward propagate the inputs, backward propagate the errors, and adjust the weights. Assume the initial weights for links coming out of units 1, 3, and 5 are 0.6, the initial weights for links coming out of units 2, 4, and 6 are 0.2, and the initial weights for links coming out of dummy unit 0 is 0.5.\n",
    "\n",
    "Fill out the math cell below with your results. You must show your work for each calculation; simply writing the value will not receive credit. The recommended way for you to do this is as follows: first, write down the general formula, evaluated as far as you can knowing everything except the index. Then, write the  unevaluated formula, and the result.\n",
    "\n",
    "Note, if you are unfamiliar with $\\LaTeX$, you can write your solutions out in plain, boring text. But we **strongly** urge you to take some time to learn the basics of $\\LaTeX$ to make some beautiful math!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ a_i := ? $$\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "a_1 = ? \\qquad\n",
    "a_2 = ? \\qquad\n",
    "a_3 = ? \\qquad\n",
    "a_4 = ? \\\\\n",
    "a_5 = ? \\qquad\n",
    "a_6 = ? \\qquad\n",
    "a_7 = ? \\qquad\n",
    "a_8 = ? \n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "$$ \\Delta_i := ? $$\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\Delta_8 = ? \\qquad\n",
    "\\Delta_7 = ? \\qquad\n",
    "\\Delta_6 = ? \\qquad\n",
    "\\Delta_5 = ? \\\\\n",
    "\\Delta_4 = ? \\qquad\n",
    "\\Delta_3 = ? \\qquad\n",
    "\\Delta_2 = ? \\qquad\n",
    "\\Delta_1 = ?  \\\\\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "\n",
    "$$ w_{i,j} := ? $$\n",
    "$$\n",
    "\\begin{gather}\n",
    "w_{0,3} = ? \\qquad\n",
    "w_{0,4} = ? \\qquad\n",
    "w_{0,5} = ? \\qquad\n",
    "w_{0,6} = ? \\qquad\n",
    "w_{0,7} = ? \\qquad\n",
    "w_{0,8} = ? \\\\\n",
    "w_{1,3} = ? \\qquad\n",
    "w_{1,4} = ? \\qquad\n",
    "w_{3,5} = ? \\qquad\n",
    "w_{3,6} = ? \\qquad\n",
    "w_{5,7} = ? \\qquad\n",
    "w_{5,8} = ? \\\\\n",
    "w_{2,3} = ? \\qquad\n",
    "w_{2,4} = ? \\qquad\n",
    "w_{4,5} = ? \\qquad\n",
    "w_{4,6} = ? \\qquad\n",
    "w_{6,7} = ? \\qquad\n",
    "w_{6,8} = ?\n",
    "\\end{gather}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> P2.  Back-Propagation Implementation </span>\n",
    "There are some modifications we will be making to the above pseudo-code as well as some assumptions:\n",
    "+ initialize the weights with random values only once, rather than every iteration of the **repeat-until**\n",
    "+ in that same vain, $\\Delta$ should only be initialized once with starting value 0 for each node\n",
    "+ random numbers should come from a uniform distribution \\[-0.5, 0.5\\] (use `random.uniform` for this)\n",
    "+ the **repeat-until** condition will be the number of epochs, given as parameter `epochs`\n",
    "+ assume `network` is fully-connected between layers\n",
    "+ $\\alpha$ refers to the learning rate, given as parameter `learning_rate`\n",
    "+ in addition to returning `network`, the function should also output a list of total mean squared error (MSE) per epoch\n",
    "  + $MSE_e = \\frac{1}{k} \\sum_j (y_j - a_j)^2$\n",
    "  + $Total\\ MSE = \\sum_e MSE_e$\n",
    "  + $e$ is indexing over examples, $k$ is the number of output units, and $j$ is indexing over the output units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your documentation here.\n",
    "\"\"\"\n",
    "def back_prop_learning(examples, network, epochs, learning_rate, deriv = sigmoid_deriv):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our NNs can learn, it's time to train and test! The code cell below will load in the 4 possible cases of the 2-bit adder as training data. After learning, we will graph how the total MSE changes over time. If you implemented the back-propagation algorithm correctly, you should see a downward trend, on average, as the total error approaches 0. You might need to run this cell a few times, as the starting weights can affect the total MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [([0,0],[0,0]), ([0,1],[0,1]), ([1,0],[0,1]), ([1,1],[1,0])]\n",
    "network = createNN(2, [2,2], 2, sigmoid)\n",
    "num_epochs = 10000\n",
    "trained_network, total_errs = back_prop_learning(dataset, network, num_epochs, 0.25)\n",
    "_, axis = plt.subplots()\n",
    "epochs = [e for e in range(0, num_epochs, 1)]\n",
    "axis.plot(epochs, total_errs)\n",
    "axis.set(xlabel = 'Number of Epochs', ylabel = 'Total Error on Training Set')\n",
    "axis.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Q3. Leaking Information </span>\n",
    "1. Why should we not test on the same data set that we used for training?\n",
    "0. Recall or look up the term **cross-validation** and explain how this process allows for both more training and testing data.\n",
    "0. Suppose that our model does not do well on the testing set, so we look at the testing set to figure out why. Once we understand why it's made mistakes, we go back and make some changes to the architecture --- maybe we delete some edges or add some nodes. When we run our experiment, it performs better! Are these results still valid?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> P3. Prediction </span>\n",
    "After the training phase comes the testing phase. To see how well our learned network performs on test data, we first need to make a prediction. In our NNs, this is done by propagating the input signals forward, and then returning the values of the output layer. Capture this logic in the function `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your documentation here.\n",
    "\"\"\"\n",
    "def predict(network, example):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can make predictions, let's first see if our NN really did learn the 2-bit adder function. The code cell below makes a prediction for each of the 4 cases and compares that to the correct outputs. On average, the learned network should get deviations very close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [[0,0], [0,1], [1,0], [1,1]]\n",
    "correct_out = [[0,0], [0,1], [0,1], [1,0]]\n",
    "for t in range(len(test_set)):\n",
    "    print('Example =', test_set[t])\n",
    "    preds = predict(trained_network, test_set[t])\n",
    "    print('Prediction =', preds)\n",
    "    print('Actual =', correct_out[t])\n",
    "    devs = [abs(preds[p] - correct_out[t][p]) for p in range(len(preds))]\n",
    "    print('Deviations =', devs)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the above only proved that our trained NN simply \"memorized\" the training set, since we are using the same data for testing. Let's see how well our back-propagation learning performs for data it has not seen. The code cell below generates 16 examples. 12 of those will randomly be used as training, and the other 4 will be used for testing. Try running this cell a few times to see how the error plot and error rate changes. You might also want to play around with the NN architecture, the number of epochs, or the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = [([p,q,r,s],[int(p and not r), int(q <= p)]) for p in [0,1] for q in [0,1] for r in [0,1] for s in [0,1]]\n",
    "random.shuffle(data_set)\n",
    "training_set = data_set[0:12]\n",
    "network = createNN(4, [8,8], 2, sigmoid)\n",
    "num_epochs = 5000\n",
    "trained_network, total_errs = back_prop_learning(training_set, network, num_epochs, 0.1)\n",
    "_, axis = plt.subplots()\n",
    "epochs = [e for e in range(0, num_epochs, 1)]\n",
    "axis.plot(epochs, total_errs)\n",
    "axis.set(xlabel = 'Number of Epochs', ylabel = 'Total Error on Training Set')\n",
    "axis.grid()\n",
    "plt.show()\n",
    "test_set = [x for (x,y) in data_set[12:]]\n",
    "correct_out = [y for (x,y) in data_set[12:]]\n",
    "num_misses = 0\n",
    "for t in range(len(test_set)):\n",
    "    preds = predict(trained_network, test_set[t])\n",
    "    devs = [abs(preds[p] - correct_out[t][p]) for p in range(len(preds))]\n",
    "    for d in devs:\n",
    "        if d >= 0.1:\n",
    "            num_misses += 1\n",
    "print('Error Rate:', num_misses / 8.0 * 100.0, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Exploring Different Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far, we have used a fixed set of hyperparameters for our NN; let's explore a bit in this section. First, let's take a look at how our choice of activation function can affect the learning phase. Below, implement the following functions as well as their derivatives. (Note, $\\alpha$ is a constant real number where applicable.)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sin(z) &= \\sin(z) &\n",
    "\\frac{\\mathrm d}{\\mathrm dz} \\sin(z) &= \\cos(z) \\\\[0.5em]\n",
    "\\tanh(z) &= \\frac{e^z - e^{-z}}{e^z + e^{-z}} &\n",
    "\\frac{\\mathrm d}{\\mathrm dz} \\tanh(z) &= 1 - \\tanh(z)^2 \\\\[0.5em]\n",
    "\\mathrm{elu}(z) &= \n",
    "  \\begin{cases}\n",
    "    z & \\text{$z > 0$} \\\\\n",
    "    \\alpha (e^z - 1) & \\text{$z \\leq 0$}\n",
    "  \\end{cases} &\n",
    "\\frac{\\mathrm d}{\\mathrm dz} \\mathrm{elu}(z) &= \n",
    "  \\begin{cases}\n",
    "    1 & \\text{$z > 0$} \\\\\n",
    "    \\alpha (e^z) & \\text{$z < 0$}\n",
    "  \\end{cases} \\\\[0.5em]\n",
    "\\mathrm{relu}(z) &= \n",
    "  \\begin{cases}\n",
    "    z & \\text{$z > 0$} \\\\\n",
    "    0 & \\text{$z \\leq 0$}\n",
    "  \\end{cases} &\n",
    "\\frac{\\mathrm d}{\\mathrm dz} \\mathrm{relu}(z) &= \n",
    "  \\begin{cases}\n",
    "    1 & \\text{$z > 0$} \\\\\n",
    "    0 & \\text{$z < 0$}\n",
    "  \\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For the trigonometric functions, it is okay to simply call the respective function from `math`. Also, for these functions, you do **not** need to document. These are relatively straight-forward, but take care with the ELU and ReLU derivatives!\n",
    "\n",
    "### <span style=\"color:red\"> P4. Alternate Activation Functions </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin(z):\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def sin_deriv(z):\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def tanh(z):\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def tanh_deriv(z):\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def elu(z, alpha = 0.01):\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def elu_deriv(z, alpha = 0.01):\n",
    "    raise NotImplementedError\n",
    "        \n",
    "\n",
    "\n",
    "def relu(z):\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def relu_deriv(z):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these different activation functions implemented, let's see how it affects the NN for our 2-bit adder data set. Try re-running this cell a few times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_set = [([0,0],[0,0]), ([0,1],[0,1]), ([1,0],[0,1]), ([1,1],[1,0])]\n",
    "learning_rate = 0.25\n",
    "num_epochs = 10000\n",
    "funs = [sigmoid, sin, tanh, elu, relu]\n",
    "drvs = [sigmoid_deriv, sin_deriv, tanh_deriv, elu_deriv, relu_deriv]\n",
    "errs = []\n",
    "r = random.getstate()\n",
    "for f in range(len(funs)):\n",
    "    network = createNN(2, [2,2], 2, funs[f])\n",
    "    random.setstate(r)\n",
    "    _, total_errs = back_prop_learning(training_set, network, num_epochs, learning_rate, deriv = drvs[f])\n",
    "    errs.append(total_errs)\n",
    "fig, axs = plt.subplots(5, 1, sharex = True, figsize = (7, 7))\n",
    "fig.subplots_adjust(hspace = 0.25)\n",
    "t = [i for i in range(0, num_epochs, 1)]\n",
    "axs[0].plot(t, errs[0], color = 'b', label = 'sigmoid')\n",
    "axs[1].plot(t, errs[1], color = 'g', label = 'sin')\n",
    "axs[2].plot(t, errs[2], color = 'r', label = 'tanh')\n",
    "axs[3].plot(t, errs[3], color = 'y', label = 'elu')\n",
    "axs[4].plot(t, errs[4], color = 'm', label = 'relu')\n",
    "axs[0].set(title = 'Total Error Over Time')\n",
    "axs[4].set(xlabel = 'Epoch')\n",
    "for i in range(len(axs)):\n",
    "    axs[i].legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other hyperparameter we will explore is the learning rate. For our 2-bit adder data set, we have used a fixed learning rate of 0.25. How does the learning rate affect back-propagation learning? Write code for `check_alphas` that will try learning rates between `min_alpha` and `max_alpha`, with a step-size of `step_size`. For each new $\\alpha$, keep track of the total MSE of the final epoch, as this is what `check_alphas` should output.\n",
    "\n",
    "### <span style=\"color:red\"> P5. Learning Rates </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your documentation here.\n",
    "\"\"\"\n",
    "def check_alphas(examples, network, epochs, min_alpha, max_alpha, step_size, deriv = sigmoid_deriv):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see how these different learning rates compare. Try different activation functions as well, to see how the choice of learning rate can affect the total error. (Be careful with ELU and ReLU!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [([0,0],[0,0]), ([0,1],[0,1]), ([1,0],[0,1]), ([1,1],[1,0])]\n",
    "network = createNN(2, [2,2], 2, sigmoid)\n",
    "num_epochs = 5000\n",
    "mina = -0.3\n",
    "maxa = 0.3\n",
    "ss = 0.05\n",
    "errs = check_alphas(dataset, network, num_epochs, mina, maxa, ss, sigmoid_deriv)\n",
    "_, axis = plt.subplots()\n",
    "alphas = []\n",
    "while mina <= maxa:\n",
    "    alphas.append(mina)\n",
    "    mina += ss\n",
    "axis.plot(alphas, errs)\n",
    "axis.set(xlabel = 'Learning Rates', ylabel = 'Total Error at Final Epoch')\n",
    "axis.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Q4. Hyperparameters </span>\n",
    "1. What conclusions can we draw about different activation functions and learning rates?\n",
    "0. Explain the term **P-hacking** and why it might be relevant here. There is a single, very commonly used term that is used in ML to refer to this: what is it?\n",
    "0. Use your answers to the previous questions to explain why in practice we partition labeled data into three subsets: a training set, a testing set, and a validation set."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will only be submitting your Jupyter notebook file, *hwk3_part1.ipynb*. Do not worry about submitting the additional files. Furthermore, as a reminder, part of your grade is your documentation. Each of the functions you implemented as part of this assignment **must** be documented. Documentation can include (but is not limited to): summary, purpose, inputs, outputs, assumptions, runtime, Python ~~hacks~~ tricks. Failure to include proper documentation will result in a penalty.\n",
    "\n",
    "Please upload your *hwk3.ipynb* file to CMS by **Friday, 12/7 @ 6pm**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
